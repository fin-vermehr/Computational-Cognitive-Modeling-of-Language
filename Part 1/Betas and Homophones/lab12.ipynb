{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## COG403: Problem 2 of Problem Set 1: Betas and Homophones\n\n### All 3 problems for Problem Set 1 Due 4 October 2018"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Imagine you are a child learning English. You are building up your vocabulary, but you are struggling with homophones (words that mean different things but sound the same). In particular, you are working on learning the difference between *for* and *four*, both of which are phonetically [fɔɹ]. (This is IPA. For more information, see https://en.wikipedia.org/wiki/International_Phonetic_Alphabet).\n\nYou assume that there is a parameter, $\\theta$, that controls how often [fɔɹ] conveys each meaning (*for* and *four*). This assumption can be formalized as the graphical model shown below. Each $X_i$ represents a sentence that uses a word that sounds like [fɔɹ]. $X_i$ takes on value 1 if [fɔɹ] in sentence $i$ means *for* and 0 if it means *four*. The assumption underlying this graphical model is that there is some unobserved value of $\\theta$ and that for each sentence, a biased coin is flipped to determine whether a word that sounds like [fɔɹ] will mean *for* (with probability $\\theta$) or mean *four* (with probability $1- \\theta$ ). You would like to learn the value of $\\theta$ by observing sentences that contain words that are pronounced [fɔɹ].\n\nWe will model learning [fɔɹ] based on corpus data.\n\n![betas graphical model](https://notebooks.azure.com/juliawatson/libraries/q2-betas-and-homophones/raw/grapical_model_betas.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (a) \n\nWrite a function to load in the Brent Ratner child-directed speech corpus and return a dictionary mapping each word type to its frequency in the corpus. This corpus is stored in `data/brent-ratner-corpus.txt` in the library for this notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from collections import Counter\n\ndef compute_word_counts(path_to_file):\n    \"\"\"\n    path_to_file: string -- the path to the corpus file\n    \n    Return a dictionary mapping each unique type in the corpus at path_to_file\n    to the number of times it occurs in the corpus. Make sure to convert words\n    to all lowercase to get unique types.\n    \"\"\"\n    \n    # Student TODO: implement this function\n    file_object = open(path_to_file, 'r')\n    \n    word_dict = dict()\n    \n    for sentence in file_object:\n        for word in sentence.split():\n            word = word.lower()\n            \n            if word not in word_dict:\n                word_dict[word] = 1\n            \n            elif word in word_dict:\n                word_dict[word] += 1\n    \n    return word_dict\n\ncorpus_counts = compute_word_counts('data/brent-ratner-corpus.txt')",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (b)\n\nThe estimate of $\\theta$ that maximizes your posterior distribution is called the MAP estimate (MAP stands for maximum a posteriori), which we refer to as $\\hat{\\theta}$. If your posterior distribution is $Beta(a_1 + k_1, a_2 + k_2)$, then you can compute $\\hat{\\theta}$ using the formula $\\frac{a_1+k_1−1}{(a_1+k_1−1)+(a_2+k_2−1)}$ . Use this formula to compute $\\hat{\\theta}$ after observing child-directed utterances below (taken from the Brent Ratner Corpus$^1$ from CHILDES$^2$):\n1. This is food **for** the dragon.\n2. One block, two blocks, three blocks, **four** blocks.\n3. Thank you **for** the phone.\n4. What do you want to get **for** your birthday?\n\nAssume an initial prior distrubution of $Beta(1, 1)$. You may do this by hand or write Python code. If you choose to do it by hand, be sure to show your work."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### (b) Solution\n\nLet $k_1$ be the number of times \"for\" is meant, and $k_2$ the number of times \"four\" is meant. In that case, $k_1 = 3, k_2 = 1$. Thus, \n\n\\begin{align*}\n\t\\hat{\\theta} &= \\frac{a_1 + k_1 - 1}{(a_1 + k_1 - 1) + (a_2 + k_2 -1)}\\\\\n\t\t\t\t &= \\frac{1 + 3 - 1}{(1 + 3 - 1) + (1 + 1 - 1)}\\\\\n\t\t\t\t &= \\frac{3}{4}\n\\end{align*}\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (c)\n\nYou have two friends, Jack and Jill, who are also trying to learn the meaning of [fɔɹ]. Their learning biases are different from yours. Jack has a prior distribution of $Beta(10,10)$ and Jill has a prior distribution of $Beta(100,100)$. After observing the utterances from part b, what are the parameters of their posterior distributions? What are their MAP estimates of $\\theta$? You may do this by hand or write Python code. If you choose to do it by hand, be sure to show your work."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### (c) Solution\n\nLet $k_1$ be the number of times \"for\" is meant, and $k_2$ the number of times \"four\" is meant. In that case, $k_1 = 3, k_2 = 1$. Jack has a posterior distribution of $Beta(13, 11)$ and a MAP estimate of $\\theta$ of: \n\n\\begin{align*}\n\t\\hat{\\theta} &= \\frac{a_1 + k_1 - 1}{(a_1 + k_1 - 1) + (a_2 +k_2 -1)}\\\\\n\t\t\t\t &= \\frac{10 + 3 - 1}{(10 + 3 - 1) + (10 + 1 - 1)}\\\\\n\t\t\t\t &= \\frac{6}{11} \\approx 0.54545\n\\end{align*}\n\t\n\nFor Jill the posterior distribution is $Beta(103, 101)$ with a a MAP estimate of $\\theta$ is: \n\n\\begin{align*}\n\t\\hat{\\theta} &= \\frac{a_1 + k_1 - 1}{(a_1 + k_1 - 1) + (a_2 +k_2 -1)}\\\\\n\t\t\t\t &= \\frac{100 + 3 - 1}{(100 + 3 - 1) + (100 + 1 - 1)}\\\\\n\t\t\t\t &= \\frac{102}{202} \\approx 0.50495\n\\end{align*}\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (d)\n\nWrite a function that uses the word frequencies computed in part a above to compute the probability of word given a list of its homophones. This probability will serve as the \"true\" $\\theta$ value -- the value that the child is seeking to learn from the sample of data they're exposed to."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def compute_theta(corpus_counts, word1, homophone_list):\n    \"\"\"\n    corpus_counts: dict of str->int, mapping words to their frequencies\n    word1: str\n    homophone_list: list of words that sound the same as word1. word1 must be in homophone_list.\n    \n    Return the probability of word1 given that a word from homophone_list occurred.\n    \"\"\"\n    # student TODO: implement this function\n    \n    count_word1 = corpus_counts[word1]\n    \n    N = 0\n    for homophone in homophone_list:\n        if homophone in corpus_counts:\n            N += corpus_counts[homophone]\n\n    return count_word1 / N\n\ntrue_theta = compute_theta(corpus_counts, \"for\", [\"for\", \"four\"])",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.9074074074074074\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this case, true theta comes to 0.9074."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (e)\n\nCompute the squared error of each of the MAP estimates (for you, Jack, and Jill) based on the true $\\theta$ computed in part d. Who had the lowest squared error: you, Jack, or Jill? You may do this by hand or write Python code. If you choose to do it by hand, be sure to show your work."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### (e) Solution\n\nThe formula for squared error is:\n$$E = (\\text{real-value - estimated-value})^2 $$\n\nThus:\n\nMy squared error is $(\\frac{196}{216} - \\frac{3}{4})^2 = 0.0248 $\n\nJack's squared error is $(\\frac{196}{216} - \\frac{6}{11})^2 = 0.131 $\n\nJill's squared error is $(\\frac{196}{216} - \\frac{102}{202})^2 = 0.162$\n\nThus I had the lowest squared error.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (f)\n\nWrite a function `generate_corpus` that creates a random corpus that you and your friends might encounter based on the theta value computed in part d. For any integer $n$, your command should return a 1-by-n vector consisting of ones (uses of [fɔɹ] that mean *for*) and zeros (uses of [fɔɹ] that mean *four*), where 1 occurs approximately $\\theta$ fraction of the time and 0 occurs approximately (1 - $\\theta$) fraction of the time. (Hint: using `numpy.random.rand(n)` will give you a vector of $n$ random numbers uniformly sampled from $[0,1)$. How can you use this to generate a list of ones and zeros where 1 occurs $\\theta$ fraction of the time?)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ndef generate_corpus(real_theta, n):\n    \"\"\"\n    n: int, the length of the generated random vector\n    \n    Return a vector filled with ones (uses of *for*) and\n    zeros (uses of *four*), where 1 occurs approximately\n    real_theta fraction of the time and 0 occurs\n    approximately (1 - real_theta) fraction of the time.\n    \"\"\"\n        \n    random_vec = np.random.rand(n)\n    \n    for index in range(len(random_vec)):\n        \n        if random_vec[index] <= real_theta:\n            random_vec[index] = 1\n        else:\n            random_vec[index] = 0\n\n    return list(random_vec)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (g)\n\nWrite a function `learn` to simulate a learner. Your function should take in a number $n$, the parameters $a_1$ and $a_2$ of the Beta prior distribution, and the true $\\theta$ value. It should first generate a random corpus of length $n$ (using `generate_corpus` from f), then use this corpus together with the prior to find the parameters of the posterior distribution, and finally use those parameters to compute the MAP estimate $\\hat{\\theta}$ and the squared error of this estimate. Your function should return the MAP estimate as well as the squared error.\n\nTest out this function by calling it using:\n * `a_1 = a_2 = 1`\n * `true_theta` = the true theta value for *for* (computed in part d)\n * `n = 100`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def learn(a1, a2, n, true_theta):\n    \"\"\"\n    a1: int -- parameter for prior Beta distribution\n    a2: int -- parameter for prior Beta distribution\n    n: int -- number of samples to use to update Beta distribution\n    true_theta: float -- the theta value we want to model. We use it to generate a corpus.\n\n    Return MAP theta value and squared error for Beta(a1, a2) after seeing n examples\n    of a word that sounds like \"for\" used to mean *for* and *four*. The examples are\n    generated randomly, with \"for\" meaning *for* true_theta fraction of the time and\n    meaning *four* (1 - true_theta) fraction of the time.\n    \"\"\"\n    # student TODO: implement this function\n    random_vector = generate_corpus(true_theta, n)\n    \n    word1_occur = random_vector.count(1) # Number of times \"for\" is meant (represented by 1 in dataset)\n    word2_occur = n - word1_occur # Number of times \"four\" is meant (represented by 0 in dataset)\n    \n    MAP_estimate = (a1 + word1_occur - 1) / ((a1 + word1_occur - 1) + (a2 + word2_occur - 1))\n    squared_error = (true_theta - MAP_estimate) ** 2\n    \n    return MAP_estimate, squared_error\n# student TODO: test out the function using the parameters listed in the question.\n\nlearn(1, 1, 100, 196/216)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "(0.9, 5.4869684499314285e-05)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "when you run learn(1, 1, 100, true\\_theta) we get MAP\\_estimate = 0.9, and squared\\_error =  0.0005104."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (h)\n\nRun an experiment to see which initial beta distribution produces the best results across multiple corpora: Write a function `evaluate_learners` that runs your simulation `learn` 1,000 times for each of five corpus sizes ($n$=1, 2, 3, 4, and 5) and for each of the three learners (you, Jack, and Jill). For each corpus size and each learner, compute the average value of $\\hat{\\theta}$ and the average squared error across the 1,000 trials, and print a summary. (To clarify: your script should run `learn` a total of 15,000 times.)\n\nIn your print statements, make sure to round any numbers to four decimal places, so that they are easily interpretable."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ndef evaluate_learners(word1, word2):\n    \"\"\"\n    Run trials for You, Jack, and Jill for different numbers of samples (1-5). Print a report of the\n    average theta and average MSE for each set of trials.\n\n    Based on the directions in the handout, assume that:\n      - You have an initial distribution of Beta(1, 1)\n      - Jack has an initial distribution of Beta(10, 10)\n      - Jill has an initial distribution of Beta(100, 100)\n\n    \"\"\"\n    # Student TODO: implement this function\n    \n    corpus_sizes = [1, 2, 3, 4, 5]\n    \n    a1_me = 1\n    a2_me = 1\n    \n    a1_jack = 10\n    a2_jack = 10\n    \n    a1_jill = 100\n    a2_jill = 100\n    \n    true_theta = compute_theta(corpus_counts, word1, [word1, word2])\n    \n    for n in corpus_sizes:\n        MAP_estimator_list = list()\n        squared_error_list = list()\n        for i in range(1, 1001):\n            learn_output = learn(a1_me, a2_me, n, true_theta)\n            MAP_estimator_list.append(learn_output[0])\n            squared_error_list.append(learn_output[1])\n        \n        print(\"Me with: n = {0}, MAP_estimator_average = {1}, squared_error_average = {2}\".format(\n        n, round(np.mean(MAP_estimator_list), 4), round(np.mean(squared_error_list), 4)))\n    \n    print('-------------------------------------------------------------------------')\n            \n    for n in corpus_sizes:\n        MAP_estimator_list = list()\n        squared_error_list = list()\n        for i in range(1, 1001):\n            learn_output = learn(a1_jack, a2_jack, n, true_theta)\n            MAP_estimator_list.append(learn_output[0])\n            squared_error_list.append(learn_output[1])\n        \n        print(\"Jack with: n = {0}, MAP_estimator_average = {1}, squared_error_average = {2}\".format(\n        n, round(np.mean(MAP_estimator_list), 4), round(np.mean(squared_error_list), 4)))\n\n    print('-------------------------------------------------------------------------')\n\n    for n in corpus_sizes:\n        MAP_estimator_list = list()\n        squared_error_list = list()\n        for i in range(1, 1001):\n            learn_output = learn(a1_jill, a2_jill, n, true_theta)\n            MAP_estimator_list.append(learn_output[0])\n            squared_error_list.append(learn_output[1])\n        \n        print(\"Jill with: n = {0}, MAP_estimator_average = {1}, squared_error_average = {2}\".format(\n        n, round(np.mean(MAP_estimator_list), 4), round(np.mean(squared_error_list), 4)))\n\nevaluate_learners('for', 'four')",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Me with: n = 1, MAP_estimator_average = 0.913, squared_error_average = 0.0795\nMe with: n = 2, MAP_estimator_average = 0.916, squared_error_average = 0.0385\nMe with: n = 3, MAP_estimator_average = 0.9047, squared_error_average = 0.0278\nMe with: n = 4, MAP_estimator_average = 0.907, squared_error_average = 0.0237\nMe with: n = 5, MAP_estimator_average = 0.9114, squared_error_average = 0.0164\n-------------------------------------------------------------------------\nJack with: n = 1, MAP_estimator_average = 0.5217, squared_error_average = 0.149\nJack with: n = 2, MAP_estimator_average = 0.5407, squared_error_average = 0.1349\nJack with: n = 3, MAP_estimator_average = 0.5581, squared_error_average = 0.1225\nJack with: n = 4, MAP_estimator_average = 0.574, squared_error_average = 0.112\nJack with: n = 5, MAP_estimator_average = 0.5885, squared_error_average = 0.1025\n-------------------------------------------------------------------------\nJill with: n = 1, MAP_estimator_average = 0.5021, squared_error_average = 0.1643\nJill with: n = 2, MAP_estimator_average = 0.504, squared_error_average = 0.1627\nJill with: n = 3, MAP_estimator_average = 0.5061, squared_error_average = 0.1611\nJill with: n = 4, MAP_estimator_average = 0.5082, squared_error_average = 0.1594\nJill with: n = 5, MAP_estimator_average = 0.5101, squared_error_average = 0.1578\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (i)\n\nRun `evaluate_learners` on *for* and *four*. For each of Me, Jack, and Jill, explain the impact of the number of samples on the mean theta and the mean squared error."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### (i) Solution\n\nThe lower both $a_1$ and $a_2$ are, the lower the squared\\_error between the MAP\\_estimator\\_average and the actual theta. This is because the ratio between 1:1 will dramatically change with a new observation, while 100:100 will barely change ($\\frac{1}{1} >>> \\frac{1}{2}$ while $\\frac{100}{100} > \\frac{101}{100}$). So the lower the parameters, the more sensitive the mean, and thus the faster the learner can converge to the eal theta. Therefore Beta(1,1) will get more closely to the true value of theta in few trials than Beta(100, 100). Also, the more trials, the lower the squared_error between the MAP_estimators and the actual theta, this is because the larger the two parameters are, the lower the variance. So if the MAP estimate mean is correct, then you want as low variance as possible, so you only generate points that are very close to the mean, otherwise you will have a large sqaured error."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (j)\n\nRun `evaluate_learners` on the homophones *too* and *two*, which are both phonetically [tu]. Which learner does best (you, Jack, or Jill)? Is this different from the results for the homophone pair (*for*, *four*)? Explain why or why not."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### (j) Solution\nOur goal is to estimate a $\\hat{\\theta}$ that maximizes our posterior distribution, with a mean that is as close as possible to $\\theta$, and has a low variance (both reduce the squared error). The formula for the variance of the Beta distribution is given by:\n$$Var(\\hat{\\theta}) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)^2}$$\nThus, the larger $\\alpha, \\beta$ the lower the variance of our MAP. In  our context, this may be understood as our certainty that our $\\alpha:\\beta$ ratio is accurate in estimating the mean. The $\\alpha : \\beta$ ratio controls the mean of $\\hat{\\theta}$:\n\nIf $\\alpha = \\beta$ then $mean(\\hat{\\theta}) = 0.5$\n\nIf $\\alpha > \\beta$, then $mean(\\hat{\\theta}) > 0.5$\n\nIf $\\alpha < \\beta$, then $mean(\\hat{\\theta}) < 0.5$\n\n\nFor our MAP estimator, the $\\alpha : \\beta$ is more quickly changed in response to a single trial if both $\\alpha$ and $\\beta$ are small. So it's easy to update the mean. The downside is that it takes a lot of trials to decrease the variance. So, if you are not confident in your prior, then it's better to have $\\alpha$ and $\\beta$ both small, so you can adjust it quickly. If however, you are confident in your prior, then you want $\\alpha$ and $\\beta$ very large to have a small variance. The downside with this is however, if your prior is wrong, it takes a lot of trials to adjust your mean.\n\n\n\nIn the previous question of comparing 'for' with 'four', the initial guesses were very wrong. The prior had a mean of $0.5$, which was very different from the actual which was approximately $0.93$, that's why my guess with a small prior most quickly approached the true mean. But, in this case, the actual $\\theta$ is quite close to $0.5$, therefore Jill's estimate was the best because she already estimated the mean accuratley with her prior, and had an MAP with very low variance.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### (k)\n\nThe human language data we give our model is very limited (just counts). What additional information do you think children use to learn the difference between homophones like *for* and *four*?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### (k) Solution\n\nWord count is pretty helpful, but our modeling of children's usage of homophones would be a lot more accurate if we incorporated the context. For example, if the sentence:\n\n\"I just drove (four / for) (four / for) miles\"\n        \n     \nand you have to estimate what means what, we would know:\n1. The probability of having either two consecutive four's is low, and the probability of having two consecutive for's is also low. Thus one of the above most likely is for, and the other is four.\n2. The probability of 'for' occurring before 'miles' is low.\n3. The probability of 'four' occurring before 'for' is low, while the probability of 'for' occurring 'four' is high.\n\nIt would also be helpful to know the syntactic structure of the sentence. 'for' is a proposition while, 'four' is a noun. If we know the structure of the entire sentence, then we can know where a proposition would be more likely than a noun.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Citations\n\n$^1$ Brent, M.R. and T.A. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. Cognition 61: 93-125.\n\n$^2$ B.MacWhinney and C. Snow. 1985. The child language data exchange system. Journal of Child Language, 12:271-296."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}