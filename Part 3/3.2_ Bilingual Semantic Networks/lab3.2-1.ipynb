{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COG403: Problem 2 of Problem Set 3: Bilingual Semantic Networks\n",
    "### The 2 problems for Problem Set 3 are due 29 Nov. 2018, 2 pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you'll connect English and Dutch monolingual semantic networks to model how bilingual Dutch-English speakers respond to a free association (FA) task.\n",
    "\n",
    "This problem draws on modeling ideas from Matusevych et al. (2018), and uses the human bilingual FA data from Van Hell & De Groot (1998) [VHDG], generously made available by the first author. \n",
    "\n",
    "**For each part of this problem, you'll find a \"to do\" list, and cells below it that indicate where to insert your code or text answer.  The cells are labeled \"Part x.n\", where x is the problem part (a, b, c, etc) and n is the numbered item from the to-do list (1, 2, etc) -- eg, \"Part a.3\".**\n",
    "\n",
    "**If, for any answer, you want to run additional code to support your answer, create a new code cell and clearly mark the answer cell that refers to it.**\n",
    "\n",
    "**References:**\n",
    "\n",
    "Matusevych, Y., Kalantari Dehaghi, A. A., & Stevenson, S. (2018). Modeling bilingual word associations as connected monolingual networks. In Proceedings of the Eighth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL) (pp. 46–56).  Association for Computational Linguistics.  https://homepages.inf.ed.ac.uk/ymatusev/publications/CMCL_2018.pdf\n",
    "\n",
    "Janet G. van Hell and Annette M. B. de Groot. 1998. Conceptual representation in bilingual memory: Effects of concreteness and cognate status in word association. Bilingualism: Language and Cognition 1(3):193–211.  (Available online through https://onesearch.library.utoronto.ca.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initialization\n",
    "\n",
    "The cell labeled Part 0 below contains the code you should run to read in the English and Dutch monolingual data from the Small World of Words (SWOW), which will create a semantic network for each language.  These two semantic networks, in `eng_graph` and `dut_graph`, will form the basis for your bilingual semantic network (which you'll join with translation links in part (a) below).\n",
    "\n",
    "The code here also reads in the human bilingual FA data from VHDG, and creates a graph structure for that data as well.  These graphs **will not be used as part of your semantic networks**, but rather will serve as \"gold standard\" data reflecting the human bilingual associations to which you'll compare the output of the bilingual semantic network you create.\n",
    "\n",
    "The data from VHDG will yield 3 graphs:\n",
    "\n",
    "* `ee_bilingual_gold`: Includes English responses of bilinguals to English cues.\n",
    "* `de_bilingual_gold`: Includes English responses of bilinguals to Dutch cues.\n",
    "* `ed_bilingual_gold`: Includes Dutch responses of bilinguals to English cues.\n",
    "\n",
    "Parts (b-c) below will consider responses in your bilingual network corresponding to the English-English data.\n",
    "\n",
    "Part (d) below will consider responses that \"cross\" languages, corresponding to the Dutch-English and English-Dutch data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1204534it [00:12, 93667.81it/s] \n",
      "1213599it [00:19, 60789.74it/s] \n",
      "2120it [00:00, 96337.21it/s]\n",
      "1584it [00:00, 43040.24it/s]\n",
      "1919it [00:00, 49700.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EE Graph for bilinguals\n",
      "{'banana': {'weight': 0.075}, 'eat': {'weight': 0.075}, 'fruit': {'weight': 0.2}, 'bite': {'weight': 0.05}, 'bean': {'weight': 0.025}, 'green': {'weight': 0.1}, 'orange': {'weight': 0.025}, 'pear': {'weight': 0.1}, 'peach': {'weight': 0.025}, 'citrus': {'weight': 0.025}, 'pie': {'weight': 0.025}, 'vegetable': {'weight': 0.025}, 'nice': {'weight': 0.025}, 'tree': {'weight': 0.225}}\n",
      "\n",
      "DE Graph for bilinguals\n",
      "{'pear': {'weight': 0.20512820512820512}, 'banana': {'weight': 0.15384615384615385}, 'green': {'weight': 0.02564102564102564}, 'eat': {'weight': 0.07692307692307693}, 'red': {'weight': 0.02564102564102564}, 'orange': {'weight': 0.02564102564102564}, 'fruit': {'weight': 0.20512820512820512}, 'tree': {'weight': 0.1794871794871795}, 'food': {'weight': 0.02564102564102564}, 'peach': {'weight': 0.05128205128205128}, 'pip': {'weight': 0.02564102564102564}}\n",
      "\n",
      "ED Graph for bilinguals\n",
      "{'sinaasappel': {'weight': 0.02564102564102564}, 'honger': {'weight': 0.02564102564102564}, 'banaan': {'weight': 0.07692307692307693}, 'groen': {'weight': 0.10256410256410256}, 'eten': {'weight': 0.05128205128205128}, 'fruit': {'weight': 0.1282051282051282}, 'vrucht': {'weight': 0.02564102564102564}, 'lekker': {'weight': 0.02564102564102564}, 'bijten': {'weight': 0.02564102564102564}, 'peer': {'weight': 0.4358974358974359}, 'boom': {'weight': 0.07692307692307693}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 0: Run this code first.\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_graph(file_path):\n",
    "    digraph = nx.DiGraph()\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.split('\\t')\n",
    "            digraph.add_edge(line[0], line[1], weight=float(line[2]))\n",
    "    return digraph\n",
    "\n",
    "eng_graph = read_graph('data/en_swow.tsv')\n",
    "dut_graph = read_graph('data/nl_swow.tsv')\n",
    "\n",
    "ee_bilingual_gold = read_graph('data/biling_data_EE.tsv')\n",
    "de_bilingual_gold = read_graph('data/biling_data_DE.tsv')\n",
    "ed_bilingual_gold = read_graph('data/biling_data_ED.tsv')\n",
    "\n",
    "print('\\nEE Graph for bilinguals')\n",
    "print(ee_bilingual_gold['apple'])\n",
    "\n",
    "print('\\nDE Graph for bilinguals')\n",
    "print(de_bilingual_gold['appel'])\n",
    "\n",
    "print('\\nED Graph for bilinguals')\n",
    "print(ed_bilingual_gold['apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a)\n",
    "\n",
    "Here, you'll create a `BilingualGraph` class and associated methods, and use those methods to join `eng_graph` and `dut_graph` into a `bilingual_graph` which forms a connected bilingual semantic network.  \n",
    "\n",
    "The `bilingual_graph` will consist of two types of edges: the **association edges** within the semantic network of each language (the edges in `eng_graph` and in `dut_graph`), and **translation edges** that you'll create here, which connect words that are 'translation equivalents' in the two languages.\n",
    "\n",
    "The weights on translation edges will be based on machine translation probabilities for English words into Dutch and Dutch words into English, provided in `data/word_alignments.csv`.\n",
    "\n",
    "In addition, translation weights between pairs of words that are automatically determined to be cognates will be upweighted, to reflect this more certain knowledge of a strong relation between the English and Dutch word pair. The cognates list is found in `data/cognates.tsv`, which provides the Levenshtein distance between pairs of English and Dutch words, as long as that distance is greater than or equal to 0.5.\n",
    "\n",
    "See the docstrings below for precise instructions on creating bilingual edges to connect the two monolingual semantic networks.\n",
    "\n",
    "**To do for Part (a):**\n",
    "\n",
    "1. Write the methods for `BilingualGraph` according to the docstrings in cell Part a.1.  **Note:** Some code is provided; you only need to fill in methods marked as \"TODO\".  Call the test cases for the `BilingualGraph` in the cell labeled \"Test Cases for Part a.1\" to ensure your code is correct before proceeding.\n",
    "\n",
    "2. Call the code in cell Part a.2 to show the results of some of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109610it [00:00, 128882.43it/s]\n",
      "109610it [00:00, 166434.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#### Part a.1  Write the methods for class BilingualGraph \n",
    "####           according to the docstrings below\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def load_cognate_links(file_path):\n",
    "    \"\"\"\n",
    "    file_path: str -- path to file containing cognate pairs\n",
    "    \n",
    "    Return a dictionary mapping a tuple of str (english_str, dutch_str) to the\n",
    "    Levenshtein distance (ratio) between them. Only word pairs with a\n",
    "    Levenshtein ratio of 0.5 or more are included.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n').split('\\t')\n",
    "            result[(line[0], line[1])] = float(line[2])\n",
    "    return result\n",
    "\n",
    "\n",
    "class BilingualGraph(object):\n",
    "    \n",
    "    def __init__(self, eng_graph, dut_graph,\n",
    "                 alignments_file_path='data/word_alignments.csv',\n",
    "                 cognates_file_path='data/cognates.tsv'):\n",
    "        \"\"\"\n",
    "        eng_graph: networkx.DirectedGraph -- graph of English SWOW cues and responses,\n",
    "            linked by association edges\n",
    "        dut_graph: networkx.DirectedGraph -- graph of Dutch SWOW cues and responses,\n",
    "            linked by association edges\n",
    "        alignments_file_path: str -- path to a csv file containing machine translation word\n",
    "            alignments. The lines in this file have the format:\n",
    "            english_word,dutch_word,english_dutch_alignment,dutch_english_alignment\n",
    "        cognates_file_path: str -- path to a tsv file containing cognate pairs and their\n",
    "            Levenshtein ratio scores. The lines in this file will have the format\n",
    "            english_word\\tdutch_word\\tlevenshtein_ratio\n",
    "        \n",
    "        Set attributes eng_graph and dut_graph to eng_graph and dut_graph passed\n",
    "        as parameters. Set attributes eng_dut_links and dut_eng_links to translation\n",
    "        edges generated by get_translation_edges.\n",
    "        \"\"\"\n",
    "        self.eng_graph = eng_graph\n",
    "        self.dut_graph = dut_graph\n",
    "        self.eng_dut_links = self.get_translation_edges(\n",
    "            alignments_file_path, cognates_file_path, 'dut')\n",
    "        self.dut_eng_links = self.get_translation_edges(\n",
    "            alignments_file_path, cognates_file_path, 'eng')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_translation_edges(self, alignments_file_path, cognates_file_path, target_lang):\n",
    "        \"\"\"\n",
    "        alignments_file_path: str -- path to a csv file containing machine translation word\n",
    "            alignments. The lines in this file have the format:\n",
    "            english_word,dutch_word,english_dutch_alignment,dutch_english_alignment\n",
    "        cognates_file_path: str -- path to a tsv file containing cognate pairs and their\n",
    "            Levenshtein ratio scores. The lines in this file will have the format\n",
    "            english_word\\tdutch_word\\tlevenshtein_ratio\n",
    "        target_lang: str in {'eng', 'dut'} -- the target language\n",
    "        \n",
    "        Return a dict mapping str to dict. The inner dict should map str to\n",
    "        float. The keys in the outer dict are strings in the source language.\n",
    "        The keys in the inner dict are strings in the target language (target_lang).\n",
    "        The values in the inner dict are the source-target translation edges.\n",
    "        \n",
    "        The source-target translation edges for a given word, source_word in the\n",
    "        source language, should be computed as follows:\n",
    "            1. Find all words in the target language that have a source-target\n",
    "                alignment probability greater than zero.\n",
    "            2. Set the translation edge weights from source_word to a given\n",
    "                target_word found in step one to:\n",
    "                    word_alignment * (1 + cognate_score)\n",
    "                where word_alignment is the source-target machine translation word\n",
    "                alignment from alignments_file_path, and cognate_score is the\n",
    "                Levenshtein distance between the source_word and target_word from\n",
    "                cognates_file_path.\n",
    "            3. Normalize source_word's outgoing translation edge scores, so that they\n",
    "                sum to one. Do this by dividing the translation edge weights from step\n",
    "                2 by the sum of the translation edge weights for all target words that\n",
    "                source_word has a translation edge to.\n",
    "        \"\"\"\n",
    "        assert target_lang in ['eng', 'dut']\n",
    "        cognate_links = load_cognate_links(cognates_file_path)\n",
    "        result = defaultdict(dict)\n",
    "        with open(alignments_file_path) as f:\n",
    "            # skip headers\n",
    "            headers = next(f)\n",
    "            for line in tqdm(f):\n",
    "                line = line.strip('\\n').split(',')\n",
    "                if len(line) != 4:\n",
    "                    print(line)\n",
    "                en = line[0]\n",
    "                nl = line[1]\n",
    "                \n",
    "                # score is product of alignments probabilities\n",
    "                if target_lang == 'eng':\n",
    "                    score = float(line[3])\n",
    "                else:\n",
    "                    score = float(line[2])\n",
    "                \n",
    "                # skip cases where score is 0\n",
    "                if score == 0:\n",
    "                    continue\n",
    "                \n",
    "                # skip cases where words are not in SWOW graphs\n",
    "                if nl not in self.dut_graph or en not in self.eng_graph:\n",
    "                    continue\n",
    "                \n",
    "                # increase weighting for cognates\n",
    "                if (en, nl) in cognate_links:\n",
    "                    score = score * (cognate_links[(en, nl)] + 1)\n",
    "                \n",
    "                if target_lang == 'eng':\n",
    "                    result[nl][en] = score\n",
    "                else:\n",
    "                    result[en][nl] = score\n",
    "                    \n",
    "        # normalize, so we get a probability distribution\n",
    "        for l1_word in result:\n",
    "            weight_sum = sum(result[l1_word].values())\n",
    "            for l2_word in result[l1_word]:\n",
    "                result[l1_word][l2_word] /= weight_sum\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def is_cue(self, language, word):\n",
    "        \"\"\"\n",
    "        language: str in {'eng', 'dut'}\n",
    "        word: str\n",
    "        \n",
    "        Return true if and only if word is a cue in the graph for language.\n",
    "        \"\"\"\n",
    "        assert language in ['eng', 'dut']\n",
    "        \n",
    "        if language == 'eng':\n",
    "            G = self.eng_graph\n",
    "        \n",
    "        elif language == 'dut':\n",
    "            G = self.dut_graph\n",
    "        \n",
    "        if G[word] != {}:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "#         return word in [node for (node, val) in G.out_degree() if val > 0]\n",
    "\n",
    "        \n",
    "    def is_translatable(self, target_language, word):\n",
    "        \"\"\"\n",
    "        target_language: str in {'eng', 'dut'}\n",
    "        word: str\n",
    "        \n",
    "        Return true if and only if word has a translation edge to a word in\n",
    "        target_language. (Note that the language of word is the source language,\n",
    "        not target_langauge.)\n",
    "        \"\"\"        \n",
    "        if target_language == 'eng':\n",
    "            target_links = self.eng_dut_links\n",
    "            source_links = self.dut_eng_links\n",
    "        else:\n",
    "            target_links = self.dut_eng_links\n",
    "            source_links = self.eng_dut_links\n",
    "        \n",
    "        translation_list = source_links[word]\n",
    "        \n",
    "        for key in translation_list:\n",
    "            if key in target_links:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "#         return any([word in links[key] for key in links])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def translate(self, target_language, word, find_cue=False):\n",
    "        \"\"\"\n",
    "        target_language: str in {'eng', 'dut'} -- the language to translate into\n",
    "        word: str -- the word to translate\n",
    "        find_cue: bool -- when set to True, the returned value must be a cue in\n",
    "            target_language\n",
    "            \n",
    "        Return a translation of word in target_language. Randomly select\n",
    "        the translation from the possible translation edges for word in\n",
    "        target_language, weighted by the translation edge weights. When\n",
    "        find_cue is set to True, restrict your search to words in target_language\n",
    "        that are cues. (Note that this means that you will need to re-normalize\n",
    "        the probabilities, since you will be considering a restricted set).\n",
    "        \n",
    "        Precondition: word must be have a translation in target_language\n",
    "        \"\"\"\n",
    "        if target_language == 'eng':\n",
    "            links = self.dut_eng_links\n",
    "        else:\n",
    "            links = self.eng_dut_links\n",
    "        \n",
    "        mapping = links[word]\n",
    "        \n",
    "        prob_list = []\n",
    "        word_list = []\n",
    "        \n",
    "        \n",
    "        for key in mapping:\n",
    "            if find_cue:\n",
    "                if self.is_cue(target_language, key):\n",
    "                    word_list.append(key)\n",
    "                    prob_list.append(mapping[key])\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                word_list.append(key)\n",
    "                prob_list.append(mapping[key])\n",
    "        \n",
    "        if find_cue:\n",
    "            normalized_prob = []\n",
    "            for i in prob_list:\n",
    "                normalized_prob.append(i / sum(prob_list))\n",
    "            prob_list = normalized_prob\n",
    "        \n",
    "        if len(word_list) == 0:\n",
    "            return None\n",
    "\n",
    "        return np.random.choice(word_list, 1, p=prob_list)[0]\n",
    "        \n",
    "    def free_association(self, language, cue, find_translatable=False):\n",
    "        \"\"\"\n",
    "        language: str -- language to do free association in\n",
    "        cue: str -- start word for free association\n",
    "        find_translatable: bool -- when set to true, the returned value must\n",
    "            have a translation from language to the other language\n",
    "            \n",
    "        Return a str found by a one-step weighed random walk from cue in \n",
    "        the graph for language. Randomly select the node for free\n",
    "        association from among cue's outgoing edges in language, weighted\n",
    "        by the association weights. When find_translatable is set to True,\n",
    "        restrict your search to words in language that have translations in\n",
    "        the other language. (Note that this means that you will need to\n",
    "        re-normalize the probabilities, since you will be considering a restricted\n",
    "        set)\n",
    "        \"\"\"\n",
    "        \n",
    "        if language is 'eng':\n",
    "            G = self.eng_graph\n",
    "            target = 'dut'\n",
    "        else:\n",
    "            G = self.dut_graph\n",
    "            target = 'eng'\n",
    "        \n",
    "        neigh_list = []\n",
    "        weight_list = []\n",
    "\n",
    "        \n",
    "        for neighbour in G[cue]:\n",
    "            if find_translatable is True:\n",
    "                if self.is_translatable(target, cue) is True:\n",
    "                    neigh_list.append(neighbour)\n",
    "                    weight_list.append(G[cue][neighbour]['weight'])\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                neigh_list.append(neighbour)\n",
    "                weight_list.append(G[cue][neighbour]['weight'])\n",
    "        if len(neigh_list) == 0:\n",
    "            return None\n",
    "        return np.random.choice(neigh_list, 1, p=weight_list)[0]\n",
    "        \n",
    "\n",
    "bilingual_graph = BilingualGraph(eng_graph, dut_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 7731.44it/s]\n",
      "6it [00:00, 4438.42it/s]\n"
     ]
    }
   ],
   "source": [
    "### TEST CASES for Part a.1\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "eng_graph_dummy = nx.DiGraph()\n",
    "eng_graph_dummy.add_edge('winter', 'pie', weight=1.0)\n",
    "eng_graph_dummy.add_edge('pumpkin', 'pie', weight=1.0)\n",
    "eng_graph_dummy.add_node('snow')\n",
    "eng_graph_dummy.add_node('apple')\n",
    "eng_graph_dummy.add_node('party')\n",
    "eng_graph_dummy.add_node('pumpkin')\n",
    "\n",
    "dut_graph_dummy = nx.DiGraph()\n",
    "dut_graph_dummy.add_edge('winter', 'sneeuw', weight=1.0)\n",
    "dut_graph_dummy.add_edge('taart', 'appel', weight=1.0)\n",
    "dut_graph_dummy.add_node('feest')\n",
    "dut_graph_dummy.add_node('partij')\n",
    "dummy_graph = BilingualGraph(\n",
    "    eng_graph_dummy, dut_graph_dummy, cognates_file_path='data/dummy_cognates.tsv',\n",
    "    alignments_file_path='data/dummy_alignments.csv')\n",
    "\n",
    "\n",
    "assert dummy_graph.is_translatable('eng', 'pie') == False\n",
    "assert dummy_graph.is_translatable('dut', 'pie') == True\n",
    "assert dummy_graph.is_translatable('dut', 'pumpkin') == False\n",
    "assert dummy_graph.is_translatable('dut', 'taart') == False\n",
    "assert dummy_graph.is_translatable('eng', 'taart') == True\n",
    "\n",
    "\n",
    "assert dummy_graph.is_cue('eng', 'winter') == True\n",
    "assert dummy_graph.is_cue('eng', 'pie') == False\n",
    "assert dummy_graph.is_cue('dut', 'winter') == True\n",
    "assert dummy_graph.is_cue('dut', 'taart') == True\n",
    "assert dummy_graph.is_cue('dut', 'sneeuw') == False\n",
    "\n",
    "\n",
    "assert dummy_graph.translate('dut', 'pie') == 'taart'\n",
    "assert dummy_graph.translate('eng', 'sneeuw') == 'snow'\n",
    "\n",
    "\n",
    "assert dummy_graph.free_association('dut', 'winter') == 'sneeuw'\n",
    "assert dummy_graph.free_association('eng', 'winter') == 'pie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dut for horoscope: horoscoop\n",
      "Eng for foefelen: fiddle\n",
      "Eng FA for penguin: bird\n",
      "Dut FA for pinguïn: noordpool\n"
     ]
    }
   ],
   "source": [
    "#### Part a.2  Call this code to show results of some of your methods.\n",
    "\n",
    "print(\"Dut for horoscope: {}\".format(bilingual_graph.translate('dut', 'horoscope')))\n",
    "print(\"Eng for foefelen: {}\".format(bilingual_graph.translate('eng', 'foefelen')))\n",
    "\n",
    "print(\"Eng FA for penguin: {}\".format(bilingual_graph.free_association('eng', 'penguin')))\n",
    "print(\"Dut FA for pinguïn: {}\".format(bilingual_graph.free_association('dut', 'pinguïn')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "Here, you'll write code to model the English-English task -- ie, where bilinguals are given cues in English, and provide responses in English.  You'll explore whether/how much implicit translation impacts the results by comparing a bilingual model to a monolingual model (ie, one that is equivalent to a monolingual English speaker).\n",
    "\n",
    "You'll model the free association task as a random walk.  Unlike in Problem Set 2.2, where a random walk had many steps so it could generate a sequence of words in a category (ie, a list of 'animals'), **here a random walk will traverse a single association link**.  If you stayed in the English network and ran the random walk over, say, 1000 trials for each English cue, then you'd get a distribution of 1000 single responses to each cue, which would match those of monolingual speakers of English.\n",
    "\n",
    "But, we're modeling bilinguals, and we suspect they don't only think in English responses to English cues!  This means they don't necessarily stay in their English network.\n",
    "\n",
    "As a second alternative, a bilingual might implicitly translate the English cue to Dutch, traverse an association link in Dutch to a Dutch response to that translated cue, and then translate the Dutch response back to English, to get the final English response.\n",
    "\n",
    "So, your random walks will have two possibilities:\n",
    "\n",
    "i. Do a random \"association\" walk from the English cue (within the English network), and output the English response.  (This kind of walk will have length 1.)\n",
    "\n",
    "ii. Do a random \"translation\" walk from the English cue into Dutch, then do a random \"association\" walk from the Dutch translation (within the Dutch network), then do a random \"translation\" walk from the Dutch response back to English, and output that English response.  (This kind of walk will have length 3.)\n",
    "\n",
    "All choices of association or translation edges in the network should be made probabilistically according to the edge weight.\n",
    "\n",
    "The choices between (i) and (ii) above will be made based on the translation probability -- a parameter to `random_walk` called `p_translate` -- which effects a simple (possibly biased) coin flip.\n",
    "\n",
    "**To do for Part (b):**\n",
    "\n",
    "1. Write function `random_walk` in code cell b.1 according to the docstring.  Call the test cases in the following cell to check your code.\n",
    "\n",
    "2. Call `random_walk` in code cell b.2 with the given parameters to show some results of calling your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Part b.1  Write function `random_walk` according to the docstring below.\n",
    "\n",
    "def random_walk(graph, start_lang, start, p_translate=0.5):\n",
    "    \"\"\"\n",
    "    graph: BilingualGraph -- the graph to use to do a random walk\n",
    "    start_lang: str in {'eng', 'dut'} -- the language to start a random walk in \n",
    "    start: str -- the word in language to start at\n",
    "    p_translate: float -- the probability of doing a \"translation\" walk (as\n",
    "        opposed to an \"association\" walk).\n",
    "    \n",
    "    Do a translation walk or an association walk, starting at start in start_lang,\n",
    "    and return the result. Randomly decide whether to do a translation or association\n",
    "    walk, giving a translation walk a weight of p_translate. The definitions of\n",
    "    association and translation walks are provided in the problem description above.\n",
    "    \n",
    "    Make sure to use the methods translate and free_association defined in the\n",
    "    BilingualGraph class, rather than re-implementing this functionality here.\n",
    "    Set find_cue and find_translatable to True where appropriate, to avoid running\n",
    "    into dead ends.\n",
    "    \"\"\"\n",
    "    \n",
    "    if start_lang == 'eng':\n",
    "        starting_graph = graph.eng_graph\n",
    "        target_lang = 'dut'\n",
    "    else:\n",
    "        starting_graph = graph.dut_graph\n",
    "        target_lang = 'eng'\n",
    "        \n",
    "    translation_walk = np.random.choice([True, False], 1, p=[p_translate, 1 - p_translate])\n",
    "    \n",
    "    flag = graph.is_translatable(target_lang, start)\n",
    "    if translation_walk and flag:\n",
    "        translation = graph.translate(target_lang, start, True)\n",
    "        next_word = graph.free_association(target_lang, translation, True)\n",
    "        return graph.translate(start_lang, next_word)\n",
    "\n",
    "    elif translation_walk and not flag:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return graph.free_association(start_lang, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE for code in part b.1\n",
    "\n",
    "assert random_walk(dummy_graph, 'eng', 'winter', p_translate=0) == 'pie'\n",
    "assert random_walk(dummy_graph, 'eng', 'winter', p_translate=1.0) == 'snow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c)\n",
    "\n",
    "Here you'll see how the model of bilingual association performs under different values of the probability of translating (`p_translate`) in the random walk.\n",
    "\n",
    "You'll run your network on the set of cues used in the VHDG experiments, doing 1000 random walk trials for each cue.  You'll repeat this for each value of `p_translate` in the random walk, varying from 0 (equivalent to a monolingual) to 1 (ie, always translating) by .1 increments.\n",
    "\n",
    "For each of the 11 sets of results you produce, you'll compare the output of your network to the distribution over responses given by human bilinguals, which are stored in  `ee_bilingual_gold` from Part 0.  We'll provide the evaluation measure for you to use: it computes a distance between two probability distributions.  This is an error measure, so **smaller numbers indicate a better match**.\n",
    "\n",
    "**To do for Part (c):**\n",
    "\n",
    "1. Write `evaluate_random_walks` in cell c.1 according to the docstring below.  Note that it will need to call the function `error` which we provide, to calculate the mismatch between the distribution of responses for each cue given by your runs of `random_walk`, and the human data in `ee_bilingual_gold`.\n",
    "\n",
    "2. Call the code in cell c.2 to generate the 11 sets of results and print them.  When you are ready to run this to analyze the results, you must call it with the default of 1000 random walks per cue (as is done in the code cell as provided).  But this can take several minutes to run, so you can try it out on smaller amounts to make sure it's working.  Smaller amounts will not give you consistent results, however, so don't count on the patterns you see there for answering the questions below.\n",
    "\n",
    "3. In cell c.3, answer the following question: What value (or range of values) for `p_translate` gives you the best performance (ie, closest match to human data)?  Explain what you think this might indicate about the nature of crosslinguistic transfer in the bilingual lexicon.  (Max 100 words.)\n",
    "\n",
    "4. Call the code in cell c.4, which will evaluate the results you've obtained, split into VHDG's cue categories of cognates vs. non-cognates, and concrete vs. abstract.  (Both these designations were manually determined for each English-Dutch cue pair by VHDG.)\n",
    "\n",
    "5. In cell c.5, answer the following question:  Compare the results of the bilingual model to the model that only uses the monolingual (English) network (ie, `p_translate` is 0).  Consider the difference between the two results for each category of cue pairs: cognate/non-cognate x concrete/abstract.  For which of the four categories of words does the bilingual model show the smallest difference from the monolingual model? the largest difference?  What does this say about the bilingual lexicon?  (Max 100 words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Part c.1: Write code for evaluate_random_walks according to the docstring below.\n",
    "\n",
    "\n",
    "def evaluate_random_walks(\n",
    "        sample, start_lang, graph, gold_standard_graph,\n",
    "        n_trials=1000, p_translate=0.5):\n",
    "    \"\"\"\n",
    "    sample: list of str -- the list of cues to use as start points for\n",
    "        random walks\n",
    "    start_lang: str -- language to start random walks in\n",
    "    graph: BilingualGraph -- graph to use for random walks\n",
    "    gold_standard_graph: BilingualGraph -- graph to use to evaluate\n",
    "        the results of the random walks\n",
    "    n_trials: int -- the number of random walks to do for each\n",
    "        cue word in sample\n",
    "    p_translate: float -- the probability of doing a translation walk\n",
    "        (as opposed to a pure association walk)\n",
    "        \n",
    "    Return a dict mapping str to float. The keys in the result should\n",
    "    be the cues in sample, and the values are the error for this cue.\n",
    "    The error for each cue in sample is calculated as follows:\n",
    "        1. Call the function random_walk from part b 1000 times, with start equal\n",
    "            to cue. You should also pass graph and start_lang. Store the resulting\n",
    "            responses.\n",
    "        2. Generate a list of tuple of (response, score), where responses\n",
    "            are unique responses from step 1. Compute the score for each\n",
    "            response by dividing that response's count by n_trials (the total\n",
    "            number of responses found).\n",
    "        3. Generate a gold standard list of tuple of (response, score), where\n",
    "            responses are the nodes that the cue has outgoing association edges\n",
    "            to in gold_standard_graph, and score is the weights of these\n",
    "            association edges.\n",
    "        4. Call error (defined below) on the lists of tuples generated in\n",
    "            steps 2 and 3.\n",
    "    \"\"\"\n",
    "    cue_dict = dict()\n",
    "\n",
    "    for cue in sample:\n",
    "        if cue is not None and graph.is_cue(start_lang, cue):\n",
    "            cue_dict[cue] = []\n",
    "            tmp_list = list()\n",
    "\n",
    "            for i in range(n_trials):\n",
    "                walk_result = random_walk(graph, start_lang, cue, p_translate)\n",
    "                if walk_result is not None:\n",
    "                    tmp_list.append(walk_result)\n",
    "\n",
    "            for element in set(tmp_list):\n",
    "                cue_dict[cue].append((element, tmp_list.count(element) / n_trials))\n",
    "\n",
    "    gold_standard_dict = dict()\n",
    "    \n",
    "    for cue in cue_dict:\n",
    "        if cue in gold_standard_graph:\n",
    "            gold_standard_dict[cue] = []\n",
    "\n",
    "            for response in gold_standard_graph[cue]:\n",
    "                gold_standard_dict[cue].append((response, gold_standard_graph[cue][response]['weight']) )\n",
    "\n",
    "    final_dict = dict()\n",
    "    \n",
    "    for cue in cue_dict:\n",
    "        if cue in gold_standard_dict:\n",
    "            error_value = error(cue_dict[cue], gold_standard_dict[cue])\n",
    "            final_dict[cue] = error_value\n",
    "    return final_dict\n",
    "\n",
    "def error(sample1, sample2):\n",
    "    \"\"\"\n",
    "    sample1: list of tuple of (str, float) -- (response, score) tuples\n",
    "    sample2: list of tuple of (str, float) -- (response, score) tuples\n",
    "    \n",
    "    Return the error of sample1 relative to sample2 based on formula (3)\n",
    "    in Matusevych et al., (2018). This uses the following procedure:\n",
    "        1. Make sure that sample1 and sample2 have the same length. If they\n",
    "            do not have the same length, make them the same length by removing\n",
    "            tuples from the longer sample. If tuples need to be removed, remove\n",
    "            the lowest weighted tuples.\n",
    "        2. Collect a list of words that are the union of the responses in\n",
    "            sample2 and sample2.\n",
    "        3. For each response in the list found in step 2, take the absolute value\n",
    "            of the difference of the scores for the response in sample1 and sample2.\n",
    "            If the response does not occur in one of the samples, default to a score\n",
    "            of 0. Sum the absolute values of the score differences over the responses.\n",
    "        3. Multiply the result from step 3 times 0.5.\n",
    "    \"\"\"\n",
    "    sample1 = sorted(sample1, key=lambda x: x[1], reverse=True)\n",
    "    sample2 = sorted(sample2, key=lambda x: x[1], reverse=True)\n",
    "    if len(sample1) > len(sample2):\n",
    "        sample1 = sample1[:len(sample2)]\n",
    "    elif len(sample2) > len(sample1):\n",
    "        sample2 = sample2[:len(sample1)]\n",
    "    sample1 = {k: v for k, v in sample1}\n",
    "    sample2 = {k: v for k, v in sample2}\n",
    "    vals = set(list(sample1.keys()) + list(sample2.keys()))\n",
    "    error = 0.5 * sum([abs(sample1.get(v, 0) - sample2.get(v, 0)) for v in vals])\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test case for part c.1\n",
    "\n",
    "ee_dummy_gold_standard_graph = nx.DiGraph()\n",
    "ee_dummy_gold_standard_graph.add_edge('winter', 'snow', weight=1.0)\n",
    "\n",
    "assert evaluate_random_walks(\n",
    "    ['winter'], 'eng', dummy_graph, ee_dummy_gold_standard_graph, p_translate=0.0) == {'winter': 1.0}\n",
    "\n",
    "assert evaluate_random_walks(\n",
    "    ['winter'], 'eng', dummy_graph, ee_dummy_gold_standard_graph, p_translate=1.0) == {'winter': 0.0}\n",
    "\n",
    "test_result = evaluate_random_walks(\n",
    "        ['winter'], 'eng', dummy_graph, ee_dummy_gold_standard_graph, p_translate=0.75)['winter']\n",
    "assert test_result < 0.14 and test_result > 0.11\n",
    "\n",
    "# not translatable\n",
    "assert evaluate_random_walks(\n",
    "    ['pumpkin'], 'eng', dummy_graph, ee_dummy_gold_standard_graph, p_translate=0.5) == {}\n",
    "\n",
    "# not a cue\n",
    "assert evaluate_random_walks(\n",
    "    ['snow'], 'eng', dummy_graph, ee_dummy_gold_standard_graph, p_translate=0.5) == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% translate mean error: 0.576\n",
      "10% translate mean error: 0.564\n",
      "20% translate mean error: 0.550\n",
      "30% translate mean error: 0.542\n",
      "40% translate mean error: 0.532\n",
      "50% translate mean error: 0.528\n",
      "60% translate mean error: 0.524\n",
      "70% translate mean error: 0.523\n",
      "80% translate mean error: 0.529\n",
      "90% translate mean error: 0.534\n",
      "100% translate mean error: 0.532\n"
     ]
    }
   ],
   "source": [
    "#### Part c.2: Call code to get cues to test, call evaluate_random_walks on\n",
    "####           each value of the translation probability, and print results.\n",
    "\n",
    "# sample of words to compute errors for -- the cues in the human EE data\n",
    "sample = [x for x in ee_bilingual_gold.nodes() if len(ee_bilingual_gold[x]) > 0]\n",
    "\n",
    "result_0 = evaluate_random_walks(\n",
    "         sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.0)\n",
    "result_10 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.1)\n",
    "result_20 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.2)\n",
    "result_30 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.3)\n",
    "result_40 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.4)\n",
    "result_50 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.5)\n",
    "result_60 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.6)\n",
    "result_70 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.7)\n",
    "result_80 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.8)\n",
    "result_90 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=0.9)\n",
    "result_100 = evaluate_random_walks(\n",
    "        sample, 'eng', bilingual_graph, ee_bilingual_gold, p_translate=1.0)\n",
    "\n",
    "# Print results:\n",
    "print(\"0% translate mean error: {:.3f}\".format(np.mean(list(result_0.values()))))\n",
    "print(\"10% translate mean error: {:.3f}\".format(np.mean(list(result_10.values()))))\n",
    "print(\"20% translate mean error: {:.3f}\".format(np.mean(list(result_20.values()))))\n",
    "print(\"30% translate mean error: {:.3f}\".format(np.mean(list(result_30.values()))))\n",
    "print(\"40% translate mean error: {:.3f}\".format(np.mean(list(result_40.values()))))\n",
    "print(\"50% translate mean error: {:.3f}\".format(np.mean(list(result_50.values()))))\n",
    "print(\"60% translate mean error: {:.3f}\".format(np.mean(list(result_60.values()))))\n",
    "print(\"70% translate mean error: {:.3f}\".format(np.mean(list(result_70.values()))))\n",
    "print(\"80% translate mean error: {:.3f}\".format(np.mean(list(result_80.values()))))\n",
    "print(\"90% translate mean error: {:.3f}\".format(np.mean(list(result_90.values()))))\n",
    "print(\"100% translate mean error: {:.3f}\".format(np.mean(list(result_100.values()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c.3: Answer the following question: What value (or range of values) for `p_translate` gives you the best performance (ie, closest match to human data)?  Explain what you think this might indicate about the nature of crosslinguistic transfer in the bilingual lexicon.  (Max 100 words.)\n",
    "\n",
    "\n",
    "Generalizing which values work best is difficuilt, especially when we are working with differently skilled bilingual participants. My understanding of crosslinguistic transfer in the bilingual lexicon leads me to believe that the occurances of this type of transfer would decrease as the participants become more skilled with both languages. Thus there is not one value to describe how often a bilingual actually uses crosslinguistic transfer that fits for all. Yet we can make a claim about averages, on average 60% - 70% translation yields the lowest error rate. This means that most of the time, bilinguals will translate the word first, do an association in their native language, and then translate the result of the association back into their second language. I hypothesize that because their second language has a smaller lexicon, it requires more effort to find associate words, while it requires less work to translate, associate and then translate again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description\tbilingual_error\tmonolingual_error\n",
      "cognate_abstract\t0.544\t0.591\n",
      "cognate_concrete\t0.476\t0.500\n",
      "non_cognate_abstract\t0.545\t0.579\n",
      "non_cognate_concrete\t0.510\t0.574\n"
     ]
    }
   ],
   "source": [
    "#### Part c.4:  Code for evaluating error on cognates vs. non-cognate cues,\n",
    "####            and abstract vs. concrete cues.\n",
    "\n",
    "def get_error_subset(word_sample, error_dict):\n",
    "    \"\"\"\n",
    "    word_sample: list of str -- list of cues. All cues must be in error_dict.\n",
    "    error_dict: dict mapping str to int -- This maps cues to error values. (Should\n",
    "        be in the format returned by evaluate_random_walks)\n",
    "    \n",
    "    Return the mean error for the words in word_sample.\n",
    "    \"\"\"\n",
    "    return np.mean([v for k, v in error_dict.items() if k in word_sample])\n",
    "\n",
    "def load_word_pairs(file_path):\n",
    "    \"\"\"\n",
    "    file_path: str -- path to file containing word pairs\n",
    "    \n",
    "    Return a list of tuple of word pairs.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            result.append(tuple(line.split()))\n",
    "    return result\n",
    "\n",
    "cognate_concrete = load_word_pairs('data/cognate_concrete.txt')\n",
    "cognate_abstract = load_word_pairs('data/cognate_abstract.txt')\n",
    "non_cognate_concrete = load_word_pairs('data/non_cognate_concrete.txt')\n",
    "non_cognate_abstract = load_word_pairs('data/non_cognate_abstract.txt')\n",
    "\n",
    "type_to_pairs = {\n",
    "    'cognate_concrete': cognate_concrete,\n",
    "    'cognate_abstract': cognate_abstract,\n",
    "    'non_cognate_concrete': non_cognate_concrete,\n",
    "    'non_cognate_abstract': non_cognate_abstract\n",
    "}\n",
    "\n",
    "print(\"description\\tbilingual_error\\tmonolingual_error\")\n",
    "for name in sorted(type_to_pairs.keys()):\n",
    "    word_pairs = type_to_pairs[name]\n",
    "    curr_sample = [x[1] for x in word_pairs]\n",
    "    curr_bi_error = get_error_subset(curr_sample, result_50)\n",
    "    curr_mon_error = get_error_subset(curr_sample, result_0)\n",
    "    print(\"{}\\t{:.3f}\\t{:.3f}\".format(name, curr_bi_error, curr_mon_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c.5:  Answer the following question:  Compare the results of the bilingual model to the model that only uses the monolingual (English) network (ie, `p_translate` is 0).  Consider the difference between the two results for each category of cue pairs: cognate/non-cognate x concrete/abstract.  For which of the four categories of words does the bilingual model show the smallest difference from the monolingual model? the largest difference?  What does this say about the bilingual lexicon?  (Max 100 words.)\n",
    "\n",
    "- Throughout the different categories, the bilingual model has a lower error than the monolingual model.\n",
    "- The bilingual model has a lower error on the non-cognates relative to the monolingual model, than on cognates. Also the bilingual model has a lower error on concrete words relative to the monolingual model than on concrete words.\n",
    "- The smallest difference is found on the cognate concrete words, while the largest difference is found on non cognante concrete words. \n",
    "- I hypothesize that we find the lowest difference on the cognate_concrete category because bilinguals and monolinguals use the same cognitive mechanism most of the time. Similar to patch switiching in word association, bilinguals will switch to the other language if it requires more effort to find an association word. In general cognates will be easier to translate (quicker translation function) and abstract concepts will be harder to translate because they are more domain specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d)\n",
    "\n",
    "In this part, you'll explore how your bilingual model does on the tasks of giving Dutch responses to English cues, and English responses to Dutch cues, comparing those results to the human data from VHDG on these tasks.\n",
    "\n",
    "We'll take a fairly simple approach here: Either the model will first translate the word from the cue language to the response language, and then do free association within the response language, or the model will first do free association in the cue language, and then translate the associate to the response language.\n",
    "\n",
    "**To do for Part (d):**\n",
    "\n",
    "1. Write `random_walk_crosslang_task` in cell d.1 according to the docstring below.  Call the test cases in the following cell to check your code.\n",
    "\n",
    "2. Write `evaluate_random_walks_crosslang_task` according to the docstring below.  We suggest you copy your code for `evaluate_random_walks` from above into cell d.2 and make suitable modifications.  Call the test cases in the following cell to check your code.\n",
    "\n",
    "3.  Call the code in cell d.3 to run your model on the crosslanguage association tasks, and print the results.\n",
    "\n",
    "4.  In cell d.4, answer the following:  Compare the performance of free associating first vs. translating first, on the English-Dutch and Dutch-English crosslanguage association tasks.  What do these results say about the bilingual lexicon? (Max 150 words.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Part d.1:  Write random_walk_crosslang_task according to the docstring below.\n",
    "\n",
    "\n",
    "def random_walk_crosslang_task(\n",
    "        graph, start_lang, start, fa_first):\n",
    "    \"\"\"\n",
    "    graph: BilingualGraph -- the graph to use to do a random walk\n",
    "    start_lang: str in {'eng', 'dut'} -- the language to start a random walk in \n",
    "    start: str -- the word in language to start at\n",
    "    fa_first: bool -- When set to True, do an association association-first walk\n",
    "        (as opposed to a translation-first walk).\n",
    "    \n",
    "    Do a translation-first or an association-first cross-language random walk,\n",
    "    starting at start in start_lang, and return the result. Randomly decide whether\n",
    "    to do a translation or association step first, giving an association-first walk\n",
    "    a weight of fa_first.  **@Julia -- fa_first is a boolean.**\n",
    "    \n",
    "    Make sure to use the methods translate and free_association defined in the\n",
    "    BilingualGraph class, rather than re-implementing this functionality here.\n",
    "    Set find_cue and find_translatable to True where appropriate, to avoid running\n",
    "    into dead ends.\n",
    "    \"\"\"\n",
    "    \n",
    "    if start_lang == 'eng':\n",
    "        target_lang = 'dut'\n",
    "    else:\n",
    "        target_lang = 'eng'\n",
    "    \n",
    "    if not fa_first:\n",
    "        translated_word = graph.translate(target_lang, start, True)\n",
    "        associated_word = graph.free_association(target_lang, translated_word, False)\n",
    "        return associated_word\n",
    "    \n",
    "    else:\n",
    "        associated_word = graph.free_association(start_lang, start, True)\n",
    "        translated_word = graph.translate(target_lang, associated_word, False)\n",
    "        return translated_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASES FOR CODE IN CELL d.1\n",
    "\n",
    "assert random_walk_crosslang_task(dummy_graph, 'eng', 'winter', fa_first=False) == 'sneeuw'\n",
    "assert random_walk_crosslang_task(dummy_graph, 'eng', 'winter', fa_first=True) == 'taart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Part d.2:  Write evaluate_random_walks_crosslang_task according to the docstring below.\n",
    "\n",
    "def evaluate_random_walks_crosslang_task(\n",
    "        sample, start_lang, graph, gold_standard_graph,\n",
    "        fa_first, n_trials=1000):\n",
    "    \"\"\"\n",
    "    sample: list of str -- the list of cues to use as start points for\n",
    "        random walks\n",
    "    start_lang: str -- language to start random walks in\n",
    "    graph: BilingualGraph -- graph to use for random walks\n",
    "    gold_standard_graph: BilingualGraph -- graph to use to evaluate\n",
    "        the results of the random walks\n",
    "    n_trials: int -- the number of random walks to do for each\n",
    "        cue word in sample\n",
    "    fa_first: bool -- when set to True, do association-first walk\n",
    "        (otherwise, do a translation-first walk)\n",
    "        \n",
    "    Return a dict mapping str to float. The keys in the result should\n",
    "    be the cues in sample, and the values are the error for this cue.\n",
    "    The error for each cue in sample is calculated as follows:\n",
    "        1. Call the function random_walk_translation_task from the cell above 1000\n",
    "            times, with start equal to cue. You should also pass graph and start_lang.\n",
    "            Store the resulting responses.\n",
    "        2. Generate a list of tuple of (response, score), where responses\n",
    "            are unique responses from step 1. Compute the score for each\n",
    "            response by dividing that response's count by n_trials (the total\n",
    "            number of responses found).\n",
    "        3. Generate a gold standard list of tuple of (response, score), where\n",
    "            responses are the nodes that the cue has outgoing association edges\n",
    "            to in gold_standard_graph, and score is the weights of these\n",
    "            association edges.\n",
    "        4. Call error (defined in part c) on the lists of tuples generated in\n",
    "            steps 2 and 3.\n",
    "    \"\"\"\n",
    "    error_dict = {}\n",
    "    \n",
    "    if start_lang == 'eng':\n",
    "        target_lang = 'dut'\n",
    "    else:\n",
    "        target_lang = 'eng'\n",
    "    \n",
    "    for cue in sample:\n",
    "        if graph.is_cue(start_lang, cue) is False or (graph.is_translatable(target_lang, cue) is False):\n",
    "            continue\n",
    "        tmp = []\n",
    "        for i in range(n_trials):\n",
    "            tmp.append(random_walk_crosslang_task(graph, start_lang, cue, fa_first))\n",
    "        \n",
    "        score_list = []\n",
    "        for i in set(tmp):\n",
    "            score_list.append((i, tmp.count(i) / n_trials))\n",
    "    \n",
    "        g_score_list = []\n",
    "        \n",
    "        for i in gold_standard_graph[cue]:\n",
    "            g_score_list.append( (i,  gold_standard_graph[cue][i]['weight']))\n",
    "        \n",
    "        error_dict[cue] = error(g_score_list, score_list)\n",
    "    return error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ding\n",
      "ding\n",
      "ding\n",
      "ding\n"
     ]
    }
   ],
   "source": [
    "## TEST CASES FOR CODE IN CELL d.2\n",
    "\n",
    "ed_dummy_gold_standard_graph = nx.DiGraph()\n",
    "ed_dummy_gold_standard_graph.add_edge('winter', 'sneeuw', weight=1.0)\n",
    "\n",
    "assert evaluate_random_walks_crosslang_task(\n",
    "    ['winter'], 'eng', dummy_graph, ed_dummy_gold_standard_graph, fa_first=False) == {'winter': 0.0}\n",
    "assert evaluate_random_walks_crosslang_task(\n",
    "    ['winter'], 'eng', dummy_graph, ed_dummy_gold_standard_graph, fa_first=True) == {'winter': 1.0}\n",
    "\n",
    "# not translatable\n",
    "assert evaluate_random_walks_crosslang_task(\n",
    "    ['pumpkin'], 'eng', dummy_graph, ed_dummy_gold_standard_graph, fa_first=False) == {}\n",
    "assert evaluate_random_walks_crosslang_task(\n",
    "    ['pumpkin'], 'eng', dummy_graph, ed_dummy_gold_standard_graph, fa_first=True) == {}\n",
    "\n",
    "# not a cue\n",
    "assert evaluate_random_walks_crosslang_task(\n",
    "    ['snow'], 'eng', dummy_graph, ed_dummy_gold_standard_graph, fa_first=False) == {}\n",
    "assert evaluate_random_walks_crosslang_task(\n",
    "    ['snow'], 'eng', dummy_graph, ed_dummy_gold_standard_graph, fa_first=True) == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ding\n",
      "ding\n",
      "ding\n",
      "ding\n",
      "ding\n",
      "ding\n",
      "ding\n",
      "ding\n",
      "FA first Eng-Dut: 0.623\n",
      "Trans first Eng-Dut: 0.559\n",
      "FA first Dut-Eng: 0.564\n",
      "Trans first Dut-Eng: 0.548\n"
     ]
    }
   ],
   "source": [
    "#### Part d.3:  Call the code below to run your model on the crosslanguage association\n",
    "####            task, and print the results.\n",
    "\n",
    "# test both hypotheses for english-dutch FA\n",
    "en_sample = [x for x in ed_bilingual_gold.nodes() if len(ed_bilingual_gold[x]) > 0] \n",
    "en_nl_fa_first = evaluate_random_walks_crosslang_task(\n",
    "    en_sample, 'eng', bilingual_graph, ed_bilingual_gold, fa_first=True)\n",
    "en_nl_trans_first = evaluate_random_walks_crosslang_task(\n",
    "    en_sample, 'eng', bilingual_graph, ed_bilingual_gold, fa_first=False)\n",
    "\n",
    "# # test both hypotheses for dutch-english FA\n",
    "nl_sample = [x for x in de_bilingual_gold.nodes() if len(de_bilingual_gold[x]) > 0]\n",
    "nl_en_fa_first = evaluate_random_walks_crosslang_task(\n",
    "    nl_sample, 'dut', bilingual_graph, de_bilingual_gold, fa_first=True)\n",
    "nl_en_trans_first = evaluate_random_walks_crosslang_task(\n",
    "    nl_sample, 'dut', bilingual_graph, de_bilingual_gold, fa_first=False)\n",
    "\n",
    "# # Print results\n",
    "print(\"FA first Eng-Dut: {:.3f}\".format(np.mean(list(en_nl_fa_first.values()))))\n",
    "print(\"Trans first Eng-Dut: {:.3f}\".format(np.mean(list(en_nl_trans_first.values()))))\n",
    "print(\"FA first Dut-Eng: {:.3f}\".format(np.mean(list(nl_en_fa_first.values()))))\n",
    "print(\"Trans first Dut-Eng: {:.3f}\".format(np.mean(list(nl_en_trans_first.values()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part d.4: Answer the following:  Compare the performance of free associating first vs. translating first, on the English-Dutch and Dutch-English crosslanguage association tasks.  What do these results say about the bilingual lexicon?  (Max 150 words.)\n",
    "\n",
    "Free associating first, in all cases performs with a higher error than translating first. Indicating that bilinguals most likely translate words first and then associate them, rather than associate them and then translate them. I have two potential explanations for this phenomena:\n",
    "\n",
    "1. The participants know that they need to find a word in the other language, thus they automatically translate the word, they reduce the search space to words that are in the other language, around the translated word.\n",
    "2. If bilinguals are asked to associate a word where the target word is in their native language, then since their native lexicon is denser than the non-native lexicon they will translate first. To answer whether this possibly explains the phenomena, further information about the participants would be needed. \n",
    " \n",
    "Interestingly the dutch to english model performed with a lower error. If there were more native english speakers, then this would be explained by explanation 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
